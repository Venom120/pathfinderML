subtask 1 (frontend):
- Generate a matrix/grid (default 10x10) where each cell can be empty, stone, fire, or diamond.
- Randomly place stones and fire in the grid, with thresholds for their counts based on the level (see below).
- Place exactly one diamond, always in the bottom right corner (9,9).
- The agent(s) always start at the top left (0,0).
- Ensure the diamond and agent are never completely surrounded (at least one open side).
- Use images from ./static for stone, fire, diamond, and agent (agent image: 'static/agent.png').
- Draw the grid with a white background.
- Add cell identifiers to the top left of each cell, numbering left-to-right, top-to-bottom.
- Construct an Agent class to support multiple agents in the grid.
- Draw all agents using their image, and support multiple agents on the grid at once.

subtask 2 (normal game features):
- Implement agent movement based on Q-learning (agents move automatically) and allow for multiple agents (default 2, can be more).
- Prevent agents from moving into 'stone' cells.
- If an agent moves into a 'fire' cell, reset its position to (0,0) and increment the attempts counter.
- When an agent reaches the 'diamond' cell:
    - Mark the agent as finished for the level (it stops moving and training).
    - When all agents reach the diamond, increment the level and generate a new matrix.
    - If the level exceeds 100, write the final score and attempts to 'winners.txt' and exit the game.
    - Reset all agents' positions to (0,0) for the new level.
- Add a menu bar at the top (height 40px, controlled by OFFSET) with:
    - "Restart" button (left): resets score, level (to 1), attempts (to 0), generates a new matrix, and resets all agents.
    - "Reset" button (center-left): resets all agents to (0,0) and restores their scores to the value at the start of the current level (does not regenerate the matrix).
    - Score counter in the center, displaying each agent's score (comma-separated).
    - Level counter on the right.
    - Attempts counter (optional, can be displayed).
- Adjust stone and fire counts by level:
    - Stones: 5 + 2 for every 20 levels (starting from level 20).
    - Fire: 5 + 2 for every 10 levels (except multiples of 20).
- Ensure the grid and menu bar are redrawn every frame to prevent flickering and keep UI updated.
- All drawing is offset by the menu bar height.

subtask 3 (Pause Feature - Single-threaded):
- Implement a pause feature toggled by the spacebar (`is_paused` flag).
- When paused:
    - Stop all agent movement and game logic except for unpausing (spacebar), quitting (ESC, window close), and menu bar drawing.
    - Draw a pause symbol (two vertical bars) in the center of the screen, using the menu bar color.
    - Apply a pixelated effect to the game area (below the menu bar) to indicate pause state.
    - The menu bar remains visible and interactive (if desired).
    - The pixelated effect is only applied once per pause/unpause cycle.

subtask 4 (implementing ML):
- Implement Q-learning for agent movement.
- Support multiple agents (default 2, can be more, e.g., 5) moving simultaneously in each step.
- Each agent learns and moves independently, but all agents act in parallel each step.
- When an agent reaches the diamond, it stops moving/training for the rest of the level.
- When all agents reach the diamond, the level increments and a new matrix is generated.
- The score for each agent is displayed in the menu bar and is not reset after each level.
- The score for each agent is incremented only if the agent has not reached the diamond.
- When "Reset" is clicked, all agents are reset to (0,0) and their scores are restored to the value at the start of the current level.
- When "Restart" is clicked, all scores, level, and attempts are reset, and a new matrix is generated.
- The 'q' and 'ESC' keys exit the game, and the spacebar toggles pause.
- The game supports up to 100 levels, and writes the final score and attempts to 'winners.txt' when finished.

subtask 5 (updating constant values to make the agent choose path from between the matrix):
- Increase reward value 10 times if the agent chooses a path that is from between the matrix and not on the boundary.
- The reward is further increased for paths in the middle of the matrix.
- Now if there is a fire/stone on s89 diagonal left up from 9,9 the agent fails to reach the diamond hence after half of bottom row the reward is not too low but still low.. same on the right right column after half the reward is low but not too low (except 9,9 cause this way the model is not choosing the diamond location).
- To address this, you can increase the Q-value (reward) for actions that move the agent closer to the diamond when the agent is in the bottom row or rightmost column, especially near the diamond. This will encourage the agent to prefer these paths even if there is a fire blocking the direct diagonal.
- This change boosts the Q-value for moves along the bottom row and rightmost column (except the diamond itself) if they bring the agent closer to the goal.
- This will help agents learn to "go around" obstacles like fire near the diagonal and still reach the diamond efficiently.
- Cells s50, s60, s70, s90, and s95 to s99 have higher reward values than s10, s20, s30, s40, and s91 to s94. This makes the agent prefer the middle and end of the bottom row/rightmost column (near the goal), while the early parts of these edges are less attractive.
